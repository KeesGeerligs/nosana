{
  "version": "0.1",
  "type": "container",
  "meta": {
    "trigger": "cli"
  },
  "ops": [
    {
      "type": "container/run",
      "id": "lm-benchmark-llama3.1",
      "args": {
        "cmd": [
          "/bin/sh", "-c", 
          "echo 'Starting lmdeploy service...'; lmdeploy serve api_server ../../root/snapshots/db1f81ad4b8c7e39777509fac66c652eb0a52f91 --model-name llama3.1 --model-format awq & sleep 30; echo 'Running lm-benchmark...'; lm-benchmark --url http://localhost:23333 --endpoint /v1/completions --use_prompt_field --job_length 60; kill -TERM 1"
        ],
        "image": "docker.io/k1llahkeezy/llm_bench_lmdeploy:0.0.1",
        "gpu": true,
        "expose": 23333,
        "env": {
          "HF_HOME": "/root/.cache/huggingface/",
          "TRANSFORMERS_CACHE": "/root/.cache/huggingface/"
        },
        "resources": [
          {
            "type": "S3",
            "url": "s3://nos-ai-models-qllsn32u/hugging-face/llama3.1/8b/4x/models--hugging-quants--Meta-Llama-3.1-8B-Instruct-AWQ-INT4",
            "target": "/root/"
          }
        ]
      }
    }
  ]
}
