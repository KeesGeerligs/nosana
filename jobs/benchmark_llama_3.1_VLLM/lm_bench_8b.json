{
  "version": "0.1",
  "type": "container",
  "meta": {
    "trigger": "cli"
  },
  "ops": [
    {
      "type": "container/run",
      "id": "VLLM-llama3.1",
      "args": {
        "entrypoint": [],
        "cmd": [
          "/bin/sh",
          "-c",
          "python3 -m vllm.entrypoints.openai.api_server --model ../../root/.cache/huggingface/llama3.1/8b/models--unsloth--Meta-Llama-3.1-8B/snapshots/069adfb3ab0ceba60b9af8f11fa51558b9f9d396 --served-model-name llama3.1_8B --max-model-len 2176> /dev/null & sleep 7m & echo \"Running lm-benchmark...\" & bench --benchmark llm --url http://localhost:8000 --ping_correction --job-length 900 --model llama3.1_8B --timeout 10 --llm-framework vllm"
        ],
        "image": "docker.io/nosana/llm_benchmark-vllm:0.0.2",
        "gpu": true,
        "resources": [
          {
            "type": "S3",
            "url": "https://models.nosana.io/hugging-face/llama3.1/8b/models--unsloth--Meta-Llama-3.1-8B",
            "target": "/root/.cache/huggingface/llama3.1/8b/models--unsloth--Meta-Llama-3.1-8B"
          }
        ]
      },
      "results": {
        "results_CU_100": "(?<=100 CU: ).*",
        "results_CU_50": "(?<=50 CU: ).*",
        "results_CU_10": "(?<=10 CU: ).*",
        "results_CU_5": "(?<=5 CU: ).*",
        "results_CU_1": "(?<=1 CU: ).*",
        "system_specs": "(?<=system specs: ).*"
      }
    }
  ]
}
